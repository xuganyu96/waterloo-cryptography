\documentclass{article}
\usepackage[margin=1in,letterpaper]{geometry}
\usepackage{amsmath,amsfonts,amssymb,amsthm}

% For source code
\usepackage{listings}

% Algorithms and pseudocode
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}

% Common shortcuts
\newcommand{\round}[1]{\lfloor {#1} \rceil}
\newcommand{\Norm}[1]{\Vert {#1} \Vert}
\newcommand{\norm}[1]{\vert {#1} \vert}
\newcommand{\var}[1]{\operatorname{Var}[{#1}]}
\newcommand{\leftsample}{\overset{{\scriptscriptstyle\$}}{\leftarrow}}

% Environments: definitions, theorems, propositions, corollaries, lemmas
%    Theorems, propositions, and definitions are numbered within the section
%    Corollaries are numbered within the theorem, though they are rarely used
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{remark}{Remark}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[theorem]


\title{ECE 612, Information Theory}
\author{Ganyu (Bruce) Xu (g66xu)}
\date{Winter, 2024}

\begin{document}
%%%% TITLE %%%%%
\maketitle

\section*{Preliminares}
\begin{definition}
    The normal distribution $N(\mu, \sigma^2)$ has the probability density function

    $$
    f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2)
    $$
\end{definition}

\begin{definition}
    The joint normal distribution $N(\mathbf{\mu}, K)$ is defined by probability density function:

    $$
    f(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n\det(K)}}
    \exp(-\frac{1}{2}
        (\mathbf{x} - \mathbf{\mu})^\intercal
        K^{-1}
        (\mathbf{x} - \mathbf{\mu})
    )
    $$
\end{definition}

\begin{theorem}[Joint normality implies marginal normality]
    If $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ follows a joint normal distribution, then any linear combination of $\mathbf{X}$ follows normal distribution.
\end{theorem}

\section{Entropy, mutual information, divergence}

\begin{theorem}
    Let $X, Y$ be random variables. $I(X; Y)$ is concave with respect to the probability distribution of X. For a fixed marignal distribution of $X$, $I(X; Y)$ is convex with respect to $f_{Y \mid X}$
\end{theorem}

\begin{theorem}[Fano's inequality]
    Let $X \rightarrow Y \rightarrow \hat{X}$ represent an encode-decode process, where $X, \hat{X} \in \mathcal{X}$ have the same support. Let $e$ denote decoding error $\hat{X} \neq X$, then:

    $$
    H(X \mid Y) \leq H(P_e) + P_e \log(\norm{\mathcal{X}})
    $$
\end{theorem}

\section{Entropy rate}

\section{Asymptotic equipartition property}

\section{Data compressions}

\section{Channel capacity}

\section{Differential entropy}
\begin{definition}
    Let $X \in \mathbb{R}$ be a random variable with probability density function $f_X$. The \textbf{differential entropy} of $X$ is defined by

    \begin{equation*}
        h(X) = -\int_{x \in \mathbb{R}} f_X(x)\ln(f_X(x))dx = -E[\ln(f_X(X))]
    \end{equation*}
\end{definition}

Some notable results:

\begin{itemize}
    \item For uniform distribution over $[0, a]$, $f_X(x) = \frac{1}{a}$, $h(X) = \ln(a)$
    \item For $X \sim N(0, \sigma^2)$, $h(X) = \frac{1}{2}\ln(2\pi e \sigma^2)$
    \item For multivariate normal $\mathbf{X} \sim N(\mathbf{0}, K)$, $h(\mathbf{X}) = \frac{1}{2}\log ((2\pi e)^n \det{K})$
    \item For some constant $a$, $h(X + a) = h(X)$
    \item For some constant $a$, $h(aX) = \ln(\norm{a}) + h(X)$
\end{itemize}

\begin{theorem}
    Let $X$ be continuous random variable with $0$ mean and $\sigma^2$ variance, then

    \begin{equation*}
        h(X) \leq \frac{1}{2}\ln(2\pi e \sigma^2)
    \end{equation*}

    Equality is reached if and only if $X$ is Gaussian
\end{theorem}

\section{Gaussian channel}
\begin{definition}[Information channel capacity]
    Let $Y = X + Z$, where $Z \leftsample N(0, \sigma^2)$ and $E[X^2] \leq P$ for some power level constraint $P$. The \textbf{information channel capacity} is defined by

    $$
    C^I = \max_{f_X : E[X^2] \leq P} I(X; Y)
    $$
\end{definition}

\begin{theorem}
    The information channel capacity of a Gaussian channel is

    \begin{equation}
        \max_{f_X: E[X^2] \leq P} I(X; Y) = \frac{1}{2}\log{(1 + \frac{P}{\sigma^2})}
    \end{equation}

    Where $P$ is the power constraint, and $\sigma^2$ is the variance of the Gaussian noise. The maximum is achieved when $X$ follows Gaussian distribution $X \sim N(0, P)$
\end{theorem}

\subsection{Parallel Gaussian Channel}
Suppose for $1 \leq l \leq n$, $Y_l = X_l + Z_l$, where $Z_l \sim N(0, N_l)$ is independent Gaussian noise, then $\mathbf{Y} = (Y_l)_{l=1}^n$ can be seen as the output of $n$ parallel Gaussian channels combined into a single channel. The capacity of the combined channel is

\begin{equation*}
    C = \max_{f_\mathbf{X}}I(\mathbf{X}; \mathbf{Y})
\end{equation*}

The optimal strategy for maximizing the capacity is to make $X_l$ independent and individually Gaussian. If there is a combined power constraint $\sum_{l=1}^nE[X_l^2] \leq P$, then \textit{power should be first given to the channel with the lowest amount of noise until the combined noise + power exceeds the next lowest noise} (\textbf{waterfilling}).

\section{Rate distortion theory}
\begin{theorem}
    Let $X$ follow $\operatorname{Bernoulli}(p)$, then

    $$
    R(D) = \begin{cases}
        h(p) - h(D)  & \text{When $D < \min(p, 1-p)$} \\
        0   &  \text{otherwise}
    \end{cases}
$$
\end{theorem}

\begin{theorem}
    For $X \leftsample N(0, \sigma^2)$:

    $$
    R(D) = \begin{cases}
        \frac{1}{2}\log(\frac{\sigma^2}{D}) & (D \leq \sigma^2) \\
        0 & \text{otherwise}
    \end{cases}
    $$
\end{theorem}
\end{document}